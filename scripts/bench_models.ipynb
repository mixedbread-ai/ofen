{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-20T23:17:27.426606Z",
     "start_time": "2024-05-20T23:17:27.422279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/juliuslipp/projects/current/mixedbread/mixedbread-ai/packages/ofen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliuslipp/projects/current/mixedbread/mixedbread-ai/packages/ofen/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from ofen.configs import DeviceConfig, OptimizationConfig\n",
    "from ofen.models import TextEncoderConfig, TextEncoder\n",
    "\n",
    "model_names = [\n",
    "    \"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "    \"mixedbread-ai/mxbai-embed-2d-large-v1\",\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"WhereIsAI/UAE-Large-V1\",\n",
    "    \"intfloat/e5-large-v2\",\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"thenlper/gte-large\",\n",
    "    \"thenlper/gte-large-zh\",\n",
    "    \"intfloat/multilingual-e5-large\",\n",
    "    \"intfloat/multilingual-e5-base\"\n",
    "]\n",
    "\n",
    "optimization_configs = [\n",
    "    OptimizationConfig(quantization=\"int8\")\n",
    "]\n",
    "\n",
    "device_config = DeviceConfig(\"cpu\")\n",
    "model_cls = TextEncoder\n",
    "model_config_cls = TextEncoderConfig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T05:18:05.118076Z",
     "start_time": "2024-05-21T05:18:03.176386Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliuslipp/projects/current/mixedbread/mixedbread-ai/packages/ofen/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "/Users/juliuslipp/projects/current/mixedbread/mixedbread-ai/packages/ofen/.venv/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:301: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:179.)\n",
      "  hidden_states = torch._nested_tensor_from_mask(hidden_states, ~attention_mask)\n",
      "[W qlinear_dynamic.cpp:247] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (128) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [64, 128, 384].  Tensor sizes: [64, 512, 1]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 21\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, cfg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(optimization_configs):\n\u001B[1;32m     14\u001B[0m     models\u001B[38;5;241m.\u001B[39mappend(model_cls(\n\u001B[1;32m     15\u001B[0m         model_name,\n\u001B[1;32m     16\u001B[0m         text_encoder_config\u001B[38;5;241m=\u001B[39mtext_encoder_config,\n\u001B[1;32m     17\u001B[0m         optimization_config\u001B[38;5;241m=\u001B[39mcfg,\n\u001B[1;32m     18\u001B[0m         device_config\u001B[38;5;241m=\u001B[39mdevice_config,\n\u001B[1;32m     19\u001B[0m     ))\n\u001B[0;32m---> 21\u001B[0m comparison_result \u001B[38;5;241m=\u001B[39m \u001B[43mBenchmarkTools\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompare_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m results[model_name] \u001B[38;5;241m=\u001B[39m comparison_result\n",
      "File \u001B[0;32m~/projects/current/mixedbread/mixedbread-ai/packages/ofen/ofen/utilities/benchmark_tools.py:58\u001B[0m, in \u001B[0;36mBenchmarkTools.compare_models\u001B[0;34m(models, pipe_input, atol)\u001B[0m\n\u001B[1;32m     56\u001B[0m first_outputs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m models:\n\u001B[0;32m---> 58\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwarmup\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, model \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(models):\n\u001B[1;32m     61\u001B[0m     mean_diff \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/projects/current/mixedbread/mixedbread-ai/packages/ofen/ofen/models/text_encoder/text_encoder.py:84\u001B[0m, in \u001B[0;36mTextEncoder.warmup\u001B[0;34m(self, batch_size, n_tokens)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwarmup\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m, batch_size: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m64\u001B[39m, n_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m WarmupResult:\n\u001B[0;32m---> 84\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mModelHelper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwarm_text_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_tokens\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/current/mixedbread/mixedbread-ai/packages/ofen/ofen/models/model_helper.py:56\u001B[0m, in \u001B[0;36mModelHelper.warm_text_model\u001B[0;34m(model, batch_size, num_tokens)\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03mWarm up a text encoder by running a batch through it.\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;124;03m    WarmupResult: The warm-up result containing times for preprocessing, forward, and postprocessing steps.\u001B[39;00m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     55\u001B[0m inp \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarm \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m num_tokens] \u001B[38;5;241m*\u001B[39m batch_size\n\u001B[0;32m---> 56\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43minp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     59\u001B[0m features \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpreprocess(inp)\n",
      "File \u001B[0;32m~/projects/current/mixedbread/mixedbread-ai/packages/ofen/ofen/models/base/base_model.py:58\u001B[0m, in \u001B[0;36mBaseModel.pipe\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124;03mPipeline method for the model. It calls the preprocess, forward, and postprocess methods in sequence.\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     55\u001B[0m \n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     57\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(features, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/projects/current/mixedbread/mixedbread-ai/packages/ofen/ofen/models/text_encoder/text_encoder.py:75\u001B[0m, in \u001B[0;36mTextEncoder.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids)\u001B[0m\n\u001B[1;32m     70\u001B[0m         token_type_ids \u001B[38;5;241m=\u001B[39m token_type_ids\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     72\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pretrained_model(\n\u001B[1;32m     73\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask, token_type_ids\u001B[38;5;241m=\u001B[39mtoken_type_ids, return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     74\u001B[0m     )\n\u001B[0;32m---> 75\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pooling_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlast_hidden_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlast_hidden_state\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\n",
      "File \u001B[0;32m~/projects/current/mixedbread/mixedbread-ai/packages/ofen/ofen/models/layers/pooling_layer.py:95\u001B[0m, in \u001B[0;36mPoolingLayer.forward\u001B[0;34m(self, attention_mask, last_hidden_state)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_mask: Tensor, last_hidden_state: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m     88\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;124;03m    Applies the pooling function to the token embeddings.\u001B[39;00m\n\u001B[1;32m     90\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;124;03m    :return: Pooled sentence embedding.\u001B[39;00m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 95\u001B[0m     sentence_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpooling_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlast_hidden_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnormalize:\n\u001B[1;32m     97\u001B[0m         sentence_embedding \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mnormalize(sentence_embedding, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/projects/current/mixedbread/mixedbread-ai/packages/ofen/ofen/models/layers/pooling_layer.py:55\u001B[0m, in \u001B[0;36mpool_mean\u001B[0;34m(token_embeddings, attention_mask)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpool_mean\u001B[39m(token_embeddings: Tensor, attention_mask: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m     54\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Pools the mean of the token embeddings, ignoring padding tokens.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 55\u001B[0m     input_mask_expanded \u001B[38;5;241m=\u001B[39m \u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken_embeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39msum(token_embeddings \u001B[38;5;241m*\u001B[39m input_mask_expanded, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m torch\u001B[38;5;241m.\u001B[39mclamp(input_mask_expanded\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;241m1\u001B[39m), \u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-9\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The expanded size of the tensor (128) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [64, 128, 384].  Tensor sizes: [64, 512, 1]"
     ]
    }
   ],
   "source": [
    "from ofen.utilities.benchmark_tools import BenchmarkTools\n",
    "\n",
    "results = {}\n",
    "for model_name in model_names:\n",
    "    text_encoder_config = model_config_cls.from_pretrained(model_name)\n",
    "    base_model = model_cls(\n",
    "        model_name,\n",
    "        text_encoder_config=text_encoder_config,\n",
    "        device_config=device_config\n",
    "    )\n",
    "\n",
    "    models = [base_model]\n",
    "    for i, cfg in enumerate(optimization_configs):\n",
    "        models.append(model_cls(\n",
    "            model_name,\n",
    "            text_encoder_config=text_encoder_config,\n",
    "            optimization_config=cfg,\n",
    "            device_config=device_config,\n",
    "        ))\n",
    "\n",
    "    comparison_result = BenchmarkTools.compare_models(models)\n",
    "    results[model_name] = comparison_result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T05:20:33.575050Z",
     "start_time": "2024-05-21T05:18:05.118910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Data input\n",
    "data = results\n",
    "\n",
    "# Prepare data for visualization\n",
    "def prepare_data_for_visualization(data):\n",
    "    records = []\n",
    "    for model, results in data.items():\n",
    "        for idx, result in enumerate(results):\n",
    "            record = {\n",
    "                'model': model,\n",
    "                'comparison': idx,\n",
    "                'mean_diff': result.mean_diff,\n",
    "                'max_diff': result.max_diff,\n",
    "                'min_diff': result.min_diff,\n",
    "                'avg_time': result.avg_time,\n",
    "                'total_time': result.total_time,\n",
    "                'is_within_range': result.is_within_range\n",
    "            }\n",
    "            records.append(record)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df = prepare_data_for_visualization(data)\n",
    "\n",
    "# Plotting the data\n",
    "def plot_comparison_results(df):\n",
    "    models = df['model'].unique()\n",
    "    n_models = len(models)\n",
    "    fig, axes = plt.subplots(n_models, 3, figsize=(18, 6 * n_models))\n",
    "\n",
    "    if n_models == 1:\n",
    "        axes = [axes]  # To make it iterable\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model_df = df[df['model'] == model]\n",
    "\n",
    "        # Plot mean, max, and min differences\n",
    "        for col, metric in enumerate(['mean_diff', 'max_diff', 'min_diff']):\n",
    "            axes[i][0].plot(\n",
    "                model_df['comparison'], model_df[metric], label=metric.replace('_', ' ').title(),\n",
    "                marker='o', linestyle='-', color='b' if metric == 'mean_diff' else 'g' if metric == 'max_diff' else 'r'\n",
    "            )\n",
    "        for index, row in model_df.iterrows():\n",
    "            if not row['is_within_range']:\n",
    "                axes[i][0].axvline(x=row['comparison'], color='k', linestyle='--')\n",
    "        axes[i][0].set_title(f'{model} - Mean, Max, and Min Differences')\n",
    "        axes[i][0].legend()\n",
    "\n",
    "        # Plot average time\n",
    "        axes[i][1].plot(model_df['comparison'], model_df['avg_time'], label='Avg Time', marker='o', linestyle='-', color='g')\n",
    "        for index, row in model_df.iterrows():\n",
    "            if not row['is_within_range']:\n",
    "                axes[i][1].axvline(x=row['comparison'], color='k', linestyle='--')\n",
    "        axes[i][1].set_title(f'{model} - Average Time')\n",
    "        axes[i][1].legend()\n",
    "\n",
    "        # Plot total time\n",
    "        axes[i][2].plot(model_df['comparison'], model_df['total_time'], label='Total Time', marker='o', linestyle='-', color='r')\n",
    "        for index, row in model_df.iterrows():\n",
    "            if not row['is_within_range']:\n",
    "                axes[i][2].axvline(x=row['comparison'], color='k', linestyle='--')\n",
    "        axes[i][2].set_title(f'{model} - Total Time')\n",
    "        axes[i][2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_comparison_results(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
